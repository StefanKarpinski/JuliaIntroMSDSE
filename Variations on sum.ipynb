{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variations on sum\n",
    "\n",
    "Very often, benchmarks are used to compare languages.  These benchmarks can lead to long discussions, first as to exactly what is being benchmarked and secondly what explains the differences.  These simple questions can sometimes get more complicated than you at first might imagine.\n",
    "\n",
    "The purpose of this notebook is for you to see a simple benchmark for yourself.  One can read the notebook and see what happened on the author's Macbook Pro with a 4-core Intel Core I7, or run the notebook yourself.\n",
    "\n",
    "(This material began life as a wonderful lecture by Steven Johnson at MIT: https://github.com/stevengj/18S096-iap17/blob/master/lecture1/Boxes-and-registers.ipynb.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `sum`: An easy enough function to understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the  **sum** function `sum(a)`, which computes\n",
    "$$\n",
    "\\mathrm{sum}(a) = \\sum_{i=1}^n a_i,\n",
    "$$\n",
    "where $n$ is the length of `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000-element Array{Float64,1}:\n",
       " 0.08953671978265532\n",
       " 0.9229089118089155\n",
       " 0.23243504664506265\n",
       " 0.6371851471468601\n",
       " 0.7169221441894971\n",
       " 0.5793056825786933\n",
       " 0.4838987204581333\n",
       " 0.1222415129438692\n",
       " 0.35036283046901895\n",
       " 0.9835303277956495\n",
       " 0.5532845489348261\n",
       " 0.9949737622543187\n",
       " 0.08664853082810353\n",
       " ⋮\n",
       " 0.3108344772643048\n",
       " 0.8904034531058518\n",
       " 0.5831932569788159\n",
       " 0.5927603341497338\n",
       " 0.8244341030682507\n",
       " 0.30322699998651537\n",
       " 0.3878957340163689\n",
       " 0.5296077850684455\n",
       " 0.9335707183215762\n",
       " 0.42470549327926177\n",
       " 0.6087827717064473\n",
       " 0.717840659068971"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = rand(10^7) # 1D vector of random numbers, uniform on [0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.023482 seconds (75.80 k allocations: 3.793 MiB)\n",
      "  0.005327 seconds (5 allocations: 176 bytes)\n",
      "  0.005942 seconds (5 allocations: 176 bytes)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.998962299000235e6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time sum(a)\n",
    "@time sum(a)\n",
    "@time sum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected result is 0.5 * 10^7, since the mean of each entry is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a few ways in a few languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia has a `BenchmarkTools.jl` package for easy and accurate benchmarking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Sum in C\n",
    "\n",
    "C is often considered the gold standard: difficult on the human, nice for the machine. Getting within a factor of 2 of C is often satisfying. Nonetheless, even within C, there are many kinds of optimizations possible that a naive C writer may or may not get the advantage of.\n",
    "\n",
    "The current author does not speak C, so he does not read the cell below, but is happy to know that you can put C code in a Julia session, compile it, and run it. Note that the `\"\"\"` wrap a multi-line string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c_sum (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_code = \"\"\"\n",
    "#include <stddef.h>\n",
    "double c_sum(size_t n, double *X) {\n",
    "    double s = 0.0;\n",
    "    for (size_t i = 0; i < n; ++i) {\n",
    "        s += X[i];\n",
    "    }\n",
    "    return s;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "const Clib = tempname()   # make a temporary file\n",
    "\n",
    "# compile to a shared library by piping C_code to gcc\n",
    "# (works only if you have gcc installed):\n",
    "\n",
    "using Libdl\n",
    "\n",
    "open(`gcc -fPIC -O3 -msse3 -xc -shared -o $(Clib * \".\" * Libdl.dlext) -`, \"w\") do f\n",
    "    print(f, C_code) \n",
    "end\n",
    "\n",
    "# define a Julia function that calls the C function:\n",
    "c_sum(X::Array{Float64}) = ccall((\"c_sum\", Clib), Float64, (Csize_t, Ptr{Float64}), length(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.998962299000687e6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sum(a) ≈ sum(a) # type \\approx and then <TAB> to get the ≈ symbolb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.516914486885071e-7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_sum(a) - sum(a)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isapprox (generic function with 8 methods)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "≈  # alias for the `isapprox` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\u001b[36m≈\u001b[39m\" can be typed by \u001b[36m\\approx<tab>\u001b[39m\n",
      "\n",
      "search: \u001b[0m\u001b[1m≈\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "isapprox(x, y; rtol::Real=atol>0 ? 0 : √eps, atol::Real=0, nans::Bool=false, norm::Function)\n",
       "\\end{verbatim}\n",
       "Inexact equality comparison: \\texttt{true} if \\texttt{norm(x-y) <= max(atol, rtol*max(norm(x), norm(y)))}. The default \\texttt{atol} is zero and the default \\texttt{rtol} depends on the types of \\texttt{x} and \\texttt{y}. The keyword argument \\texttt{nans} determines whether or not NaN values are considered equal (defaults to false).\n",
       "\n",
       "For real or complex floating-point values, if an \\texttt{atol > 0} is not specified, \\texttt{rtol} defaults to the square root of \\href{@ref}{\\texttt{eps}} of the type of \\texttt{x} or \\texttt{y}, whichever is bigger (least precise). This corresponds to requiring equality of about half of the significand digits. Otherwise, e.g. for integer arguments or if an \\texttt{atol > 0} is supplied, \\texttt{rtol} defaults to zero.\n",
       "\n",
       "\\texttt{x} and \\texttt{y} may also be arrays of numbers, in which case \\texttt{norm} defaults to the usual \\texttt{norm} function in LinearAlgebra, but may be changed by passing a \\texttt{norm::Function} keyword argument. (For numbers, \\texttt{norm} is the same thing as \\texttt{abs}.) When \\texttt{x} and \\texttt{y} are arrays, if \\texttt{norm(x-y)} is not finite (i.e. \\texttt{±Inf} or \\texttt{NaN}), the comparison falls back to checking whether all elements of \\texttt{x} and \\texttt{y} are approximately equal component-wise.\n",
       "\n",
       "The binary operator \\texttt{≈} is equivalent to \\texttt{isapprox} with the default arguments, and \\texttt{x ≉ y} is equivalent to \\texttt{!isapprox(x,y)}.\n",
       "\n",
       "Note that \\texttt{x ≈ 0} (i.e., comparing to zero with the default tolerances) is equivalent to \\texttt{x == 0} since the default \\texttt{atol} is \\texttt{0}.  In such cases, you should either supply an appropriate \\texttt{atol} (or use \\texttt{norm(x) ≤ atol}) or rearrange your code (e.g. use \\texttt{x ≈ y} rather than \\texttt{x - y ≈ 0}).   It is not possible to pick a nonzero \\texttt{atol} automatically because it depends on the overall scaling (the \"units\") of your problem: for example, in \\texttt{x - y ≈ 0}, \\texttt{atol=1e-9} is an absurdly small tolerance if \\texttt{x} is the \\href{https://en.wikipedia.org/wiki/Earth_radius}{radius of the Earth} in meters, but an absurdly large tolerance if \\texttt{x} is the \\href{https://en.wikipedia.org/wiki/Bohr_radius}{radius of a Hydrogen atom} in meters.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> 0.1 ≈ (0.1 - 1e-10)\n",
       "true\n",
       "\n",
       "julia> isapprox(10, 11; atol = 2)\n",
       "true\n",
       "\n",
       "julia> isapprox([10.0^9, 1.0], [10.0^9, 2.0])\n",
       "true\n",
       "\n",
       "julia> 1e-10 ≈ 0\n",
       "false\n",
       "\n",
       "julia> isapprox(1e-10, 0, atol=1e-8)\n",
       "true\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "isapprox(x, y; rtol::Real=atol>0 ? 0 : √eps, atol::Real=0, nans::Bool=false, norm::Function)\n",
       "```\n",
       "\n",
       "Inexact equality comparison: `true` if `norm(x-y) <= max(atol, rtol*max(norm(x), norm(y)))`. The default `atol` is zero and the default `rtol` depends on the types of `x` and `y`. The keyword argument `nans` determines whether or not NaN values are considered equal (defaults to false).\n",
       "\n",
       "For real or complex floating-point values, if an `atol > 0` is not specified, `rtol` defaults to the square root of [`eps`](@ref) of the type of `x` or `y`, whichever is bigger (least precise). This corresponds to requiring equality of about half of the significand digits. Otherwise, e.g. for integer arguments or if an `atol > 0` is supplied, `rtol` defaults to zero.\n",
       "\n",
       "`x` and `y` may also be arrays of numbers, in which case `norm` defaults to the usual `norm` function in LinearAlgebra, but may be changed by passing a `norm::Function` keyword argument. (For numbers, `norm` is the same thing as `abs`.) When `x` and `y` are arrays, if `norm(x-y)` is not finite (i.e. `±Inf` or `NaN`), the comparison falls back to checking whether all elements of `x` and `y` are approximately equal component-wise.\n",
       "\n",
       "The binary operator `≈` is equivalent to `isapprox` with the default arguments, and `x ≉ y` is equivalent to `!isapprox(x,y)`.\n",
       "\n",
       "Note that `x ≈ 0` (i.e., comparing to zero with the default tolerances) is equivalent to `x == 0` since the default `atol` is `0`.  In such cases, you should either supply an appropriate `atol` (or use `norm(x) ≤ atol`) or rearrange your code (e.g. use `x ≈ y` rather than `x - y ≈ 0`).   It is not possible to pick a nonzero `atol` automatically because it depends on the overall scaling (the \"units\") of your problem: for example, in `x - y ≈ 0`, `atol=1e-9` is an absurdly small tolerance if `x` is the [radius of the Earth](https://en.wikipedia.org/wiki/Earth_radius) in meters, but an absurdly large tolerance if `x` is the [radius of a Hydrogen atom](https://en.wikipedia.org/wiki/Bohr_radius) in meters.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> 0.1 ≈ (0.1 - 1e-10)\n",
       "true\n",
       "\n",
       "julia> isapprox(10, 11; atol = 2)\n",
       "true\n",
       "\n",
       "julia> isapprox([10.0^9, 1.0], [10.0^9, 2.0])\n",
       "true\n",
       "\n",
       "julia> 1e-10 ≈ 0\n",
       "false\n",
       "\n",
       "julia> isapprox(1e-10, 0, atol=1e-8)\n",
       "true\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  isapprox(x, y; rtol::Real=atol>0 ? 0 : √eps, atol::Real=0, nans::Bool=false, norm::Function)\u001b[39m\n",
       "\n",
       "  Inexact equality comparison: \u001b[36mtrue\u001b[39m if \u001b[36mnorm(x-y) <= max(atol,\n",
       "  rtol*max(norm(x), norm(y)))\u001b[39m. The default \u001b[36matol\u001b[39m is zero and the default \u001b[36mrtol\u001b[39m\n",
       "  depends on the types of \u001b[36mx\u001b[39m and \u001b[36my\u001b[39m. The keyword argument \u001b[36mnans\u001b[39m determines\n",
       "  whether or not NaN values are considered equal (defaults to false).\n",
       "\n",
       "  For real or complex floating-point values, if an \u001b[36matol > 0\u001b[39m is not specified,\n",
       "  \u001b[36mrtol\u001b[39m defaults to the square root of \u001b[36meps\u001b[39m of the type of \u001b[36mx\u001b[39m or \u001b[36my\u001b[39m, whichever is\n",
       "  bigger (least precise). This corresponds to requiring equality of about half\n",
       "  of the significand digits. Otherwise, e.g. for integer arguments or if an\n",
       "  \u001b[36matol > 0\u001b[39m is supplied, \u001b[36mrtol\u001b[39m defaults to zero.\n",
       "\n",
       "  \u001b[36mx\u001b[39m and \u001b[36my\u001b[39m may also be arrays of numbers, in which case \u001b[36mnorm\u001b[39m defaults to the\n",
       "  usual \u001b[36mnorm\u001b[39m function in LinearAlgebra, but may be changed by passing a\n",
       "  \u001b[36mnorm::Function\u001b[39m keyword argument. (For numbers, \u001b[36mnorm\u001b[39m is the same thing as\n",
       "  \u001b[36mabs\u001b[39m.) When \u001b[36mx\u001b[39m and \u001b[36my\u001b[39m are arrays, if \u001b[36mnorm(x-y)\u001b[39m is not finite (i.e. \u001b[36m±Inf\u001b[39m or\n",
       "  \u001b[36mNaN\u001b[39m), the comparison falls back to checking whether all elements of \u001b[36mx\u001b[39m and \u001b[36my\u001b[39m\n",
       "  are approximately equal component-wise.\n",
       "\n",
       "  The binary operator \u001b[36m≈\u001b[39m is equivalent to \u001b[36misapprox\u001b[39m with the default arguments,\n",
       "  and \u001b[36mx ≉ y\u001b[39m is equivalent to \u001b[36m!isapprox(x,y)\u001b[39m.\n",
       "\n",
       "  Note that \u001b[36mx ≈ 0\u001b[39m (i.e., comparing to zero with the default tolerances) is\n",
       "  equivalent to \u001b[36mx == 0\u001b[39m since the default \u001b[36matol\u001b[39m is \u001b[36m0\u001b[39m. In such cases, you should\n",
       "  either supply an appropriate \u001b[36matol\u001b[39m (or use \u001b[36mnorm(x) ≤ atol\u001b[39m) or rearrange your\n",
       "  code (e.g. use \u001b[36mx ≈ y\u001b[39m rather than \u001b[36mx - y ≈ 0\u001b[39m). It is not possible to pick a\n",
       "  nonzero \u001b[36matol\u001b[39m automatically because it depends on the overall scaling (the\n",
       "  \"units\") of your problem: for example, in \u001b[36mx - y ≈ 0\u001b[39m, \u001b[36matol=1e-9\u001b[39m is an\n",
       "  absurdly small tolerance if \u001b[36mx\u001b[39m is the radius of the Earth\n",
       "  (https://en.wikipedia.org/wiki/Earth_radius) in meters, but an absurdly\n",
       "  large tolerance if \u001b[36mx\u001b[39m is the radius of a Hydrogen atom\n",
       "  (https://en.wikipedia.org/wiki/Bohr_radius) in meters.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> 0.1 ≈ (0.1 - 1e-10)\u001b[39m\n",
       "\u001b[36m  true\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> isapprox(10, 11; atol = 2)\u001b[39m\n",
       "\u001b[36m  true\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> isapprox([10.0^9, 1.0], [10.0^9, 2.0])\u001b[39m\n",
       "\u001b[36m  true\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> 1e-10 ≈ 0\u001b[39m\n",
       "\u001b[36m  false\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> isapprox(1e-10, 0, atol=1e-8)\u001b[39m\n",
       "\u001b[36m  true\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?≈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now benchmark the C code directly from Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     9.080 ms (0.00% GC)\n",
       "  median time:      10.187 ms (0.00% GC)\n",
       "  mean time:        10.268 ms (0.00% GC)\n",
       "  maximum time:     15.446 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          487\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_bench = @benchmark c_sum($a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: Fastest time was 9.080271 msec\n"
     ]
    }
   ],
   "source": [
    "println(\"C: Fastest time was $(minimum(c_bench.times) / 1e6) msec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 1 entry:\n",
       "  \"C\" => 9.08027"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dict()  # a \"dictionary\", i.e. an associative array\n",
    "d[\"C\"] = minimum(c_bench.times) / 1e6  # in milliseconds\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Python's built in `sum` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PyCall` package provides a Julia interface to Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using PyCall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <built-in function sum>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call a low-level PyCall function to get a Python list, because\n",
    "# by default PyCall will convert to a NumPy array instead (we benchmark NumPy below):\n",
    "\n",
    "apy_list = PyCall.array2py(a)\n",
    "\n",
    "# get the Python built-in \"sum\" function:\n",
    "pysum = pybuiltin(\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.998962299000687e6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysum(a) ≈ sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.516914486885071e-7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pysum(a) - sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  48 bytes\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     43.773 ms (0.00% GC)\n",
       "  median time:      47.439 ms (0.00% GC)\n",
       "  mean time:        47.979 ms (0.00% GC)\n",
       "  maximum time:     63.981 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          105\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_list_bench = @benchmark $pysum($apy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 2 entries:\n",
       "  \"C\"               => 9.08027\n",
       "  \"Python built-in\" => 43.7733"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"Python built-in\"] = minimum(py_list_bench.times) / 1e6\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Python: `numpy` \n",
    "\n",
    "`numpy` is an optimized C library, callable from Python.\n",
    "It may be installed within Julia as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "using Conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  48 bytes\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     3.364 ms (0.00% GC)\n",
       "  median time:      3.896 ms (0.00% GC)\n",
       "  mean time:        4.099 ms (0.00% GC)\n",
       "  maximum time:     10.147 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1217\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np = pyimport(\"numpy\")\n",
    "apy_numpy = PyObject(a) # converts to a numpy array by default\n",
    "\n",
    "py_numpy_bench = @benchmark $(np.sum)($apy_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: numpy_sum not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: numpy_sum not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[22]:1"
     ]
    }
   ],
   "source": [
    "numpy_sum(apy_list) # python thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: numpy_sum not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: numpy_sum not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[23]:1"
     ]
    }
   ],
   "source": [
    "numpy_sum(apy_list) ≈ sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: numpy_sum not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: numpy_sum not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[24]:1"
     ]
    }
   ],
   "source": [
    "numpy_sum(apy_list) - sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 3 entries:\n",
       "  \"C\"               => 9.08027\n",
       "  \"Python numpy\"    => 3.3641\n",
       "  \"Python built-in\" => 43.7733"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"Python numpy\"] = minimum(py_numpy_bench.times) / 1e6\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Python, hand-written "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <function py_sum at 0x1393ee3b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py\"\"\"\n",
    "def py_sum(a):\n",
    "    s = 0.0\n",
    "    for x in a:\n",
    "        s = s + x\n",
    "    return s\n",
    "\"\"\"\n",
    "\n",
    "sum_py = py\"py_sum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  48 bytes\n",
       "  allocs estimate:  3\n",
       "  --------------\n",
       "  minimum time:     199.998 ms (0.00% GC)\n",
       "  median time:      210.346 ms (0.00% GC)\n",
       "  mean time:        214.474 ms (0.00% GC)\n",
       "  maximum time:     256.654 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          24\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_hand = @benchmark $sum_py($apy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "sum(a::<b>AbstractArray</b>; <i>dims</i>) in Base at <a href=\"https://github.com/JuliaLang/julia/tree/2c187bddb8c67a6565109a3249348fa0058e7f27/base/reducedim.jl#L652\" target=\"_blank\">reducedim.jl:652</a>"
      ],
      "text/plain": [
       "sum(a::AbstractArray; dims) in Base at reducedim.jl:652"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which sum([1.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.998962299000687e6"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_py(apy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_py(apy_list) ≈ sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.516914486885071e-7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_py(apy_list) - sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 4 entries:\n",
       "  \"C\"                   => 9.08027\n",
       "  \"Python numpy\"        => 3.3641\n",
       "  \"Python hand-written\" => 199.998\n",
       "  \"Python built-in\"     => 43.7733"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"Python hand-written\"] = minimum(py_hand.times) / 1e6\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Julia (built-in) \n",
    "\n",
    "## Written directly in Julia, not in C!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "sum(a::<b>AbstractArray</b>; <i>dims</i>) in Base at <a href=\"https://github.com/JuliaLang/julia/tree/2c187bddb8c67a6565109a3249348fa0058e7f27/base/reducedim.jl#L652\" target=\"_blank\">reducedim.jl:652</a>"
      ],
      "text/plain": [
       "sum(a::AbstractArray; dims) in Base at reducedim.jl:652"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     3.601 ms (0.00% GC)\n",
       "  median time:      4.217 ms (0.00% GC)\n",
       "  mean time:        4.242 ms (0.00% GC)\n",
       "  maximum time:     6.246 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1176\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_bench = @benchmark sum($a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 5 entries:\n",
       "  \"C\"                   => 9.08027\n",
       "  \"Python numpy\"        => 3.3641\n",
       "  \"Python hand-written\" => 199.998\n",
       "  \"Python built-in\"     => 43.7733\n",
       "  \"Julia built-in\"      => 3.60092"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"Julia built-in\"] = minimum(j_bench.times) / 1e6\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Julia (hand-written) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mysum (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mysum(A)   \n",
    "    s = 0.0 # s = zero(eltype(A))\n",
    "    for a in A\n",
    "        s += a\n",
    "    end\n",
    "    s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     9.324 ms (0.00% GC)\n",
       "  median time:      10.387 ms (0.00% GC)\n",
       "  mean time:        10.620 ms (0.00% GC)\n",
       "  maximum time:     17.604 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          471\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_bench_hand = @benchmark mysum($a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 6 entries:\n",
       "  \"C\"                   => 9.08027\n",
       "  \"Python numpy\"        => 3.3641\n",
       "  \"Julia hand-written\"  => 9.32369\n",
       "  \"Python hand-written\" => 199.998\n",
       "  \"Python built-in\"     => 43.7733\n",
       "  \"Julia built-in\"      => 3.60092"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"Julia hand-written\"] = minimum(j_bench_hand.times) / 1e6\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Julia (hand-written + simd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mysum_simd (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mysum_simd(A)   \n",
    "    s = 0.0 # s = zero(eltype(A))\n",
    "    @simd for a in A\n",
    "        s += a\n",
    "    end\n",
    "    s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     3.633 ms (0.00% GC)\n",
       "  median time:      4.121 ms (0.00% GC)\n",
       "  mean time:        4.171 ms (0.00% GC)\n",
       "  maximum time:     6.972 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          1196\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j_bench_hand_simd = @benchmark mysum_simd($a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.998962299000221e6"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysum_simd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 7 entries:\n",
       "  \"Julia hand-written simd\" => 3.63319\n",
       "  \"C\"                       => 9.08027\n",
       "  \"Python numpy\"            => 3.3641\n",
       "  \"Julia hand-written\"      => 9.32369\n",
       "  \"Python hand-written\"     => 199.998\n",
       "  \"Python built-in\"         => 43.7733\n",
       "  \"Julia built-in\"          => 3.60092"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"Julia hand-written simd\"] = minimum(j_bench_hand_simd.times) / 1e6\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python numpy              3.4\n",
      "Julia built-in            3.6\n",
      "Julia hand-written simd   3.6\n",
      "C                         9.1\n",
      "Julia hand-written        9.3\n",
      "Python built-in          43.8\n",
      "Python hand-written     200.0\n"
     ]
    }
   ],
   "source": [
    "for (key, value) in sort(collect(d), by=last)\n",
    "    println(rpad(key, 23, \" \"), lpad(round(value, digits=1), 6, \" \"))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.4.0-DEV",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "212px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "2",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
